{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chamiln17/TLM-AUP/blob/main/AUP_Pretrained_YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3a0NuUtr8UG",
        "outputId": "107900a0-8902-4e2d-9b0f-35f93da4b038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.75)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: efficientnet-pytorch in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics opencv-python-headless efficientnet-pytorch albumentations pandas scikit-learn openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fQy2nyA3rg23"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import yaml\n",
        "from difflib import get_close_matches\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations import augmentations\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ultralytics import YOLO\n",
        "import random\n",
        "\n",
        "def standardize_product_name(name):\n",
        "    \"\"\"Standardize product names for matching\"\"\"\n",
        "    # Convert to lowercase\n",
        "    name = name.lower()\n",
        "\n",
        "    # Remove special characters and standardize spaces\n",
        "    name = re.sub(r'[_\\-\\(\\)]', ' ', name)\n",
        "\n",
        "    # Remove volume indicators\n",
        "    name = re.sub(r'\\d+\\s*[cl]l?', '', name)\n",
        "    name = re.sub(r'\\d+\\s*ml', '', name)\n",
        "    name = re.sub(r'\\d+\\s*L', '', name)\n",
        "\n",
        "    # Remove specific words that might vary\n",
        "    name = re.sub(r'\\b(pet|pack|cannete|ramy)\\b', '', name)\n",
        "\n",
        "    # Clean up extra spaces\n",
        "    name = ' '.join(name.split())\n",
        "\n",
        "    return name.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRf3ektaxjD-",
        "outputId": "37dcc72d-b6c5-4da7-8743-a03c08001d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print('Drive mounted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yUsP64v5sPRB"
      },
      "outputs": [],
      "source": [
        "class DataAugmenter:\n",
        "    def __init__(self, input_folder, output_folder, num_augmentations=5):\n",
        "        self.input_folder = Path(input_folder)\n",
        "        self.output_folder = Path(output_folder)\n",
        "        self.num_augmentations = num_augmentations\n",
        "\n",
        "        # Optimized augmentation pipeline for shelf products\n",
        "        self.transform = A.Compose([\n",
        "            # Lighting variations (most important for shelf products)\n",
        "            A.OneOf([\n",
        "                # Simulate different store lighting conditions\n",
        "                A.RandomBrightnessContrast(\n",
        "                    brightness_limit=(-0.3, 0.3),  # Increased range for store lighting\n",
        "                    contrast_limit=(-0.2, 0.2),    # Moderate contrast variation\n",
        "                    p=0.8\n",
        "                ),\n",
        "                # Simulate fluorescent lighting color variations\n",
        "                A.ColorJitter(\n",
        "                    brightness=0.3,\n",
        "                    contrast=0.2,\n",
        "                    saturation=0.1,  # Reduced saturation variation\n",
        "                    hue=0.05,        # Minimal hue changes\n",
        "                    p=0.8\n",
        "                ),\n",
        "            ], p=0.9),  # High probability of lighting augmentation\n",
        "\n",
        "            # Shadows (common in shelf environments)\n",
        "            A.RandomShadow(\n",
        "                shadow_roi=(0, 0, 1, 0.5),  # Shadows mainly in upper half\n",
        "                num_shadows_lower=1,\n",
        "                num_shadows_upper=2,\n",
        "                shadow_dimension=4,\n",
        "                p=0.4\n",
        "            ),\n",
        "\n",
        "            # Minimal geometric transformations (products are usually upright)\n",
        "            A.OneOf([\n",
        "                A.HorizontalFlip(p=0.5),  # Products can be viewed from either side\n",
        "                A.ShiftScaleRotate(\n",
        "                    shift_limit=0.05,      # Minimal shift\n",
        "                    scale_limit=0.1,       # Slight scale variation\n",
        "                    rotate_limit=5,        # Very minimal rotation\n",
        "                    p=0.5\n",
        "                ),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Shelf-specific degradations\n",
        "            A.OneOf([\n",
        "                # Simulate slightly out-of-focus photos\n",
        "                A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
        "                # Simulate lower light conditions\n",
        "                A.MultiplicativeNoise(\n",
        "                    multiplier=[0.8, 1.2],\n",
        "                    per_channel=False,\n",
        "                    p=0.2\n",
        "                ),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Perspective variations (viewing angles on shelves)\n",
        "            A.OneOf([\n",
        "                A.Perspective(\n",
        "                    scale=(0.02, 0.05),    # Reduced perspective change\n",
        "                    p=0.3\n",
        "                ),\n",
        "                A.OpticalDistortion(\n",
        "                    distort_limit=0.05,    # Minimal distortion\n",
        "                    shift_limit=0.05,\n",
        "                    p=0.3\n",
        "                ),\n",
        "            ], p=0.2),\n",
        "        ])\n",
        "\n",
        "    def augment_dataset(self):\n",
        "        self.output_folder.mkdir(exist_ok=True, parents=True)\n",
        "        total_images = len(list(self.input_folder.glob('*.png')))\n",
        "        processed = 0\n",
        "\n",
        "        for img_path in self.input_folder.glob('*.png'):\n",
        "            try:\n",
        "                image = cv2.imread(str(img_path))\n",
        "                if image is None:\n",
        "                    print(f\"Warning: Could not read image {img_path}\")\n",
        "                    continue\n",
        "\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Save original image\n",
        "                original_output_path = self.output_folder / img_path.name\n",
        "                Image.fromarray(image).save(original_output_path)\n",
        "\n",
        "                # Generate augmented versions with progress tracking\n",
        "                for i in range(self.num_augmentations):\n",
        "                    augmented = self.transform(image=image)['image']\n",
        "                    aug_filename = f\"{img_path.stem}_aug_{i}{img_path.suffix}\"\n",
        "                    aug_output_path = self.output_folder / aug_filename\n",
        "                    Image.fromarray(augmented).save(aug_output_path)\n",
        "\n",
        "                processed += 1\n",
        "                if processed % 10 == 0:  # Progress update every 10 images\n",
        "                    print(f\"Processed {processed}/{total_images} images ({(processed/total_images)*100:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    def verify_augmentations(self):\n",
        "        \"\"\"Verify that augmented images are valid and readable\"\"\"\n",
        "        print(\"\\nVerifying augmented images...\")\n",
        "        invalid_images = []\n",
        "        total_images = len(list(self.output_folder.glob('*.png')))\n",
        "        verified = 0\n",
        "\n",
        "        for img_path in self.output_folder.glob('*.png'):\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                img.verify()\n",
        "                img.close()\n",
        "                verified += 1\n",
        "                if verified % 50 == 0:  # Progress update every 50 verifications\n",
        "                    print(f\"Verified {verified}/{total_images} images ({(verified/total_images)*100:.1f}%)\")\n",
        "            except Exception as e:\n",
        "                invalid_images.append((img_path, str(e)))\n",
        "\n",
        "        if invalid_images:\n",
        "            print(\"\\nFound invalid images:\")\n",
        "            for path, error in invalid_images:\n",
        "                print(f\"- {path}: {error}\")\n",
        "        else:\n",
        "            print(\"\\nAll augmented images are valid!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "L9ryKgRfsE1d"
      },
      "outputs": [],
      "source": [
        "class ProductClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ProductClassifier, self).__init__()\n",
        "        # Change to efficientnet-b7\n",
        "        self.base_model = EfficientNet.from_pretrained('efficientnet-b2')\n",
        "        self.features = self.base_model.extract_features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(0.3)  # Increased 0.5 for B7 , 0.3 b2\n",
        "        # Change the input features from 1280 b0 to 2560 (B7's feature size) 1408 b2\n",
        "        self.classifier = nn.Linear(1408, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x).squeeze(-1).squeeze(-1)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "    def save_model(self, path, epoch, optimizer, scheduler, best_val_loss):\n",
        "        \"\"\"Save model checkpoint with additional training information\"\"\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'best_val_loss': best_val_loss\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path, optimizer=None, scheduler=None):\n",
        "        \"\"\"Load model checkpoint with additional training information\"\"\"\n",
        "        checkpoint = torch.load(path)\n",
        "        self.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scheduler and checkpoint['scheduler_state_dict']:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        return checkpoint['epoch'], checkpoint['best_val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KKPugiSgseyU"
      },
      "outputs": [],
      "source": [
        "class ProductDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AKNL-31oskN1"
      },
      "outputs": [],
      "source": [
        "class RamyProductDetector:\n",
        "    def __init__(self, augmented_folder, products_file, model_path=None):\n",
        "        # Update the detection model initialization to use fine-tuned weights if available\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.detection_model = YOLO(model_path)\n",
        "        else:\n",
        "            self.detection_model = YOLO('yolov8x.pt')\n",
        "        self.classification_model = None\n",
        "        self.augmented_folder = Path(augmented_folder)\n",
        "        self.products_df = pd.read_excel(products_file)\n",
        "        self.class_mapping = {}\n",
        "        self.inv_class_mapping = {}\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((260,260)),  # Increased from 224x224 , b2 260\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self, epochs=30, batch_size=16):\n",
        "        print(\"Preparing training data...\")\n",
        "        data = []\n",
        "\n",
        "        # Create standardized product to family mapping from Excel file\n",
        "        product_family_map = {}\n",
        "        for index, row in self.products_df.iterrows():\n",
        "            std_name = standardize_product_name(row['Produit'])\n",
        "            product_family_map[std_name] = row['Famille']\n",
        "            product_family_map[row['Produit']] = row['Famille']\n",
        "\n",
        "        # Create family to index mapping with Others class\n",
        "        unique_families = sorted(self.products_df['Famille'].unique())\n",
        "        unique_families.append('Others')  # Add Others class\n",
        "        self.family_mapping = {family: idx for idx, family in enumerate(unique_families)}\n",
        "        self.inv_family_mapping = {idx: family for family, idx in self.family_mapping.items()}\n",
        "\n",
        "        print(f\"Found {len(self.family_mapping)} unique families (including Others) in Excel file\")\n",
        "\n",
        "\n",
        "        # Keep track of matching results\n",
        "        products_not_found = set()\n",
        "        products_matched = {}\n",
        "        families_found = set()\n",
        "        image_extensions = ['*.png', '*.jpg']\n",
        "        for ext in image_extensions:\n",
        "          for img_path in self.augmented_folder.glob(ext):\n",
        "              product_name = img_path.stem.split('_aug_')[0]\n",
        "              std_product_name = standardize_product_name(product_name)\n",
        "\n",
        "              family = None\n",
        "              matched_name = None\n",
        "\n",
        "              # Try exact match first\n",
        "              if product_name in product_family_map:\n",
        "                  family = product_family_map[product_name]\n",
        "                  matched_name = product_name\n",
        "              elif std_product_name in product_family_map:\n",
        "                  family = product_family_map[std_product_name]\n",
        "                  matched_name = std_product_name\n",
        "              else:\n",
        "                  # Try fuzzy matching\n",
        "                  excel_names = [standardize_product_name(p) for p in self.products_df['Produit']]\n",
        "                  matches = get_close_matches(std_product_name, excel_names, n=1, cutoff=0.8)\n",
        "\n",
        "                  if matches:\n",
        "                      matched_std_name = matches[0]\n",
        "                      for orig_name in self.products_df['Produit']:\n",
        "                          if standardize_product_name(orig_name) == matched_std_name:\n",
        "                              family = product_family_map[orig_name]\n",
        "                              matched_name = orig_name\n",
        "                              break\n",
        "\n",
        "              if family is not None:\n",
        "                  families_found.add(family)\n",
        "                  family_idx = self.family_mapping[family]\n",
        "                  data.append((img_path, family_idx))\n",
        "                  products_matched[product_name] = matched_name\n",
        "              else:\n",
        "                  # Add to Others class for training\n",
        "                  others_idx = self.family_mapping['Others']\n",
        "                  data.append((img_path, others_idx))\n",
        "                  products_not_found.add(product_name)\n",
        "\n",
        "        # Print detailed report\n",
        "        print(\"\\nMatching Report:\")\n",
        "        print(\"================\")\n",
        "        print(f\"Total images processed: {len(data) + len(products_not_found)}\")\n",
        "        print(f\"Successfully matched: {len(data)}\")\n",
        "        print(f\"Unmatched: {len(products_not_found)}\")\n",
        "        print(f\"\\nUnique families found: {len(families_found)}\")\n",
        "        print(\"Families and their products:\")\n",
        "\n",
        "        family_products = {}\n",
        "        for prod, matched in products_matched.items():\n",
        "            fam = product_family_map[matched]\n",
        "            if fam not in family_products:\n",
        "                family_products[fam] = []\n",
        "            family_products[fam].append(prod)\n",
        "\n",
        "        for family, products in family_products.items():\n",
        "            print(f\"\\n{family}:\")\n",
        "            for prod in sorted(products):\n",
        "                print(f\"  - {prod} -> {products_matched[prod]}\")\n",
        "\n",
        "        print(\"\\nUnmatched products:\")\n",
        "        for prod in sorted(products_not_found):\n",
        "            print(f\"  - {prod}\")\n",
        "\n",
        "        if not data:\n",
        "            raise ValueError(\"No valid data found for training! Check product names and Excel file.\")\n",
        "\n",
        "        # Create inverse mapping for later use\n",
        "        self.inv_family_mapping = {v: k for k, v in self.family_mapping.items()}\n",
        "\n",
        "        # Split data into train and validation sets\n",
        "        train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(f\"\\nTraining set size: {len(train_data)}\")\n",
        "        print(f\"Validation set size: {len(val_data)}\")\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_dataset = ProductDataset(train_data, self.transform)\n",
        "        val_dataset = ProductDataset(val_data, self.transform)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        # Initialize model with number of families instead of products\n",
        "        self.classification_model = ProductClassifier(len(self.family_mapping))\n",
        "\n",
        "        # Train the model\n",
        "        self._train_model(train_loader, val_loader, epochs)\n",
        "\n",
        "    def _train_model(self, train_loader, val_loader, epochs):\n",
        "        print(\"Training classification model...\")\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.classification_model.to(device)\n",
        "\n",
        "        # Calculate class weights for balanced training\n",
        "        all_labels = []\n",
        "        for _, labels in train_loader:\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "        class_counts = np.bincount(all_labels)\n",
        "        total_samples = len(all_labels)\n",
        "        class_weights = torch.FloatTensor(total_samples / (len(class_counts) * class_counts))\n",
        "        class_weights = class_weights.to(device)\n",
        "\n",
        "        # Use weighted loss for imbalanced classes\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        optimizer = optim.AdamW(self.classification_model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.2)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        model_save_path = 'model_checkpoints'\n",
        "        os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "        # Add data augmentation for minority classes\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.Resize((260, 260)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase with weighted sampling\n",
        "            self.classification_model.train()\n",
        "            train_loss = 0\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Apply augmentation to minority classes\n",
        "                if random.random() < 0.5:  # 50% chance to apply augmentation\n",
        "                    inputs = torch.stack([train_transform(img) for img in inputs])\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.classification_model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation phase\n",
        "            self.classification_model.eval()\n",
        "            val_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            class_correct = torch.zeros(len(self.family_mapping))\n",
        "            class_total = torch.zeros(len(self.family_mapping))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = self.classification_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    # Per-class accuracy\n",
        "                    for i in range(len(self.family_mapping)):\n",
        "                        mask = labels == i\n",
        "                        class_correct[i] += (predicted[mask] == labels[mask]).sum().item()\n",
        "                        class_total[i] += mask.sum().item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Save model checkpoint\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                self.classification_model.save_model(\n",
        "                    os.path.join(model_save_path, f'best_model_epoch_{epoch+1}.pth'),\n",
        "                    epoch + 1,\n",
        "                    optimizer,\n",
        "                    scheduler,\n",
        "                    best_val_loss\n",
        "                )\n",
        "\n",
        "            # Print training statistics\n",
        "            print(f'Epoch {epoch+1}/{epochs}')\n",
        "            print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
        "            print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "            print(f'Overall Val Accuracy: {100*correct/total:.2f}%')\n",
        "\n",
        "            # Print per-class accuracy\n",
        "            print(\"\\nPer-class validation accuracy:\")\n",
        "            for family, idx in self.family_mapping.items():\n",
        "                if class_total[idx] > 0:\n",
        "                    accuracy = 100 * class_correct[idx] / class_total[idx]\n",
        "                    print(f'{family}: {accuracy:.2f}%')\n",
        "\n",
        "        # Load best model at the end of training\n",
        "        self.classification_model.load_model(\n",
        "            os.path.join(model_save_path, f'best_model_epoch_{epoch+1}.pth'),\n",
        "            optimizer,\n",
        "            scheduler\n",
        "        )\n",
        "\n",
        "    def detect_products(self, image_folder, output_folder):\n",
        "      print(\"Detecting and classifying products...\")\n",
        "      results_list = []\n",
        "      Path(output_folder).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "      # Handle multiple image formats\n",
        "      image_paths = []\n",
        "      image_paths.extend(Path(image_folder).glob('*.png'))\n",
        "      image_paths.extend(Path(image_folder).glob('*.jpg'))\n",
        "      image_paths.extend(Path(image_folder).glob('*.jpeg'))\n",
        "\n",
        "      for img_path in image_paths:\n",
        "          image = cv2.imread(str(img_path))\n",
        "          if image is None:\n",
        "              print(f\"Warning: Could not read image {img_path}\")\n",
        "              continue\n",
        "\n",
        "          detections = self.detection_model(image)[0]\n",
        "\n",
        "          for det in detections.boxes.data:\n",
        "              x1, y1, x2, y2, conf, _ = det\n",
        "              if conf > 0.1:  # YOLO confidence threshold\n",
        "                  roi = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "                  if roi.size == 0:\n",
        "                      continue\n",
        "\n",
        "                  try:\n",
        "                      classification = self.classify_product(roi)\n",
        "\n",
        "                      # Define color based on classification\n",
        "                      color = (0, 255, 0) if classification['family'] != 'Others' else (0, 0, 255)\n",
        "\n",
        "                      results_list.append({\n",
        "                          'image': img_path.name,\n",
        "                          'product': classification['product'],\n",
        "                          'family': classification['family'],\n",
        "                          'confidence': float(conf),\n",
        "                          'classification_confidence': classification.get('confidence', 0.0),\n",
        "                          'x1': int(x1),\n",
        "                          'y1': int(y1),\n",
        "                          'x2': int(x2),\n",
        "                          'y2': int(y2)\n",
        "                      })\n",
        "\n",
        "                      # Draw bounding box and label\n",
        "                      cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "                      label = f\"{classification['family']} ({classification.get('confidence', 0.0):.2f})\"\n",
        "                      cv2.putText(image, label, (int(x1), int(y1)-10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "                  except Exception as e:\n",
        "                      print(f\"Error classifying ROI: {e}\")\n",
        "                      continue\n",
        "\n",
        "          output_path = os.path.join(output_folder, f\"annotated_{img_path.name}\")\n",
        "          cv2.imwrite(output_path, image)\n",
        "\n",
        "      return pd.DataFrame(results_list)\n",
        "\n",
        "    def classify_product(self, roi_image):\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.classification_model.eval()\n",
        "\n",
        "        roi_image = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
        "        roi_tensor = self.transform(roi_image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.classification_model(roi_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "            predicted_idx = predicted.item()\n",
        "            confidence = confidence.item()\n",
        "\n",
        "        if predicted_idx not in self.inv_family_mapping:\n",
        "            print(f\"Warning: Predicted index {predicted_idx} not found in mapping\")\n",
        "            return {'product': 'Unknown', 'family': 'Unknown', 'confidence': 0.0}\n",
        "\n",
        "        family = self.inv_family_mapping[predicted_idx]\n",
        "\n",
        "        try:\n",
        "            # Get all products from this family\n",
        "            family_products = self.products_df[self.products_df['Famille'] == family]['Produit']\n",
        "\n",
        "            if family_products.empty:\n",
        "                print(f\"Warning: No products found for family {family}\")\n",
        "                return {'product': 'Unknown', 'family': family, 'confidence': confidence}\n",
        "\n",
        "            # Select the first available product as representative\n",
        "            product = family_products.iloc[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error selecting product for family {family}: {e}\")\n",
        "            return {'product': 'Unknown', 'family': family, 'confidence': confidence}\n",
        "\n",
        "        return {\n",
        "            'product': product,\n",
        "            'family': family,\n",
        "            'confidence': confidence\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "class ProductAnalyzer:\n",
        "    def __init__(self, detection_results_df):\n",
        "        self.results_df = detection_results_df\n",
        "\n",
        "    def analyze_product_appearances(self):\n",
        "        \"\"\"Analyze product appearances per image and generate comparison metrics\"\"\"\n",
        "        analysis_results = {}\n",
        "\n",
        "        # Group detections by image\n",
        "        image_groups = self.results_df.groupby('image')\n",
        "\n",
        "        for image_name, image_detections in image_groups:\n",
        "            # Initialize counters for this image\n",
        "            ramy_products = defaultdict(int)\n",
        "            competitor_products = defaultdict(int)\n",
        "            total_products = len(image_detections)\n",
        "\n",
        "            # Count products\n",
        "            for _, detection in image_detections.iterrows():\n",
        "                family = detection['family']\n",
        "                if family != 'Others':  # Ramy product\n",
        "                    ramy_products[family] += 1\n",
        "                else:\n",
        "                    competitor_products['Others'] += 1\n",
        "\n",
        "            # Calculate statistics\n",
        "            total_ramy = sum(ramy_products.values())\n",
        "            total_competitors = sum(competitor_products.values())\n",
        "\n",
        "            # Prepare detailed analysis for this image\n",
        "            image_analysis = {\n",
        "                \"total_products_detected\": total_products,\n",
        "                \"ramy_products\": {\n",
        "                    \"total\": total_ramy,\n",
        "                    \"percentage\": (total_ramy / total_products * 100) if total_products > 0 else 0,\n",
        "                    \"by_family\": {\n",
        "                        family: {\n",
        "                            \"count\": count,\n",
        "                            \"percentage\": (count / total_products * 100) if total_products > 0 else 0\n",
        "                        }\n",
        "                        for family, count in ramy_products.items()\n",
        "                    }\n",
        "                },\n",
        "                \"competitor_products\": {\n",
        "                    \"total\": total_competitors,\n",
        "                    \"percentage\": (total_competitors / total_products * 100) if total_products > 0 else 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "            analysis_results[image_name] = image_analysis\n",
        "\n",
        "        return analysis_results\n",
        "\n",
        "    def generate_summary_stats(self, analysis_results):\n",
        "        \"\"\"Generate overall summary statistics\"\"\"\n",
        "        total_detections = 0\n",
        "        total_ramy = 0\n",
        "        family_totals = defaultdict(int)\n",
        "        total_competitors = 0\n",
        "\n",
        "        for image_analysis in analysis_results.values():\n",
        "            total_detections += image_analysis[\"total_products_detected\"]\n",
        "            total_ramy += image_analysis[\"ramy_products\"][\"total\"]\n",
        "            total_competitors += image_analysis[\"competitor_products\"][\"total\"]\n",
        "\n",
        "            for family, data in image_analysis[\"ramy_products\"][\"by_family\"].items():\n",
        "                family_totals[family] += data[\"count\"]\n",
        "\n",
        "        summary = {\n",
        "            \"overall_statistics\": {\n",
        "                \"total_products_detected\": total_detections,\n",
        "                \"total_ramy_products\": total_ramy,\n",
        "                \"total_competitor_products\": total_competitors,\n",
        "                \"ramy_market_share\": (total_ramy / total_detections * 100) if total_detections > 0 else 0,\n",
        "                \"competitor_market_share\": (total_competitors / total_detections * 100) if total_detections > 0 else 0\n",
        "            },\n",
        "            \"ramy_family_distribution\": {\n",
        "                family: {\n",
        "                    \"total\": count,\n",
        "                    \"percentage\": (count / total_ramy * 100) if total_ramy > 0 else 0\n",
        "                }\n",
        "                for family, count in family_totals.items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def export_to_json(self, output_path):\n",
        "        \"\"\"Generate and export the complete analysis to JSON\"\"\"\n",
        "        analysis_results = self.analyze_product_appearances()\n",
        "        summary_stats = self.generate_summary_stats(analysis_results)\n",
        "\n",
        "        complete_analysis = {\n",
        "            \"summary\": summary_stats,\n",
        "            \"per_image_analysis\": analysis_results\n",
        "        }\n",
        "\n",
        "        # Export to JSON with proper formatting\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(complete_analysis, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return complete_analysis\n",
        "\n",
        "def analyze_detections_and_export(detection_results_path, output_json_path):\n",
        "    \"\"\"Convenience function to analyze detections and export results\"\"\"\n",
        "    # Read detection results\n",
        "    detection_results = pd.read_excel(detection_results_path)\n",
        "\n",
        "    # Create analyzer and export results\n",
        "    analyzer = ProductAnalyzer(detection_results)\n",
        "    analysis_results = analyzer.export_to_json(output_json_path)\n",
        "\n",
        "    print(f\"Analysis complete. Results saved to {output_json_path}\")\n",
        "    return analysis_results"
      ],
      "metadata": {
        "id": "yb_NgpBtqMKd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8yNNfcbtsxGe"
      },
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "base_dir = '/content/drive/MyDrive/AUP'\n",
        "reference_folder = os.path.join(base_dir, 'Ramy')\n",
        "augmented_folder = os.path.join(base_dir, 'augmented')\n",
        "rayonnage_folder = os.path.join(base_dir, 'Rayonnage')\n",
        "output_folder = os.path.join(base_dir, 'output')\n",
        "products_file = os.path.join(base_dir, 'Liste_Produit_Ramy1.xlsx')\n",
        "yolo_model_path = os.path.join(base_dir, 'train/weights/best.pt')  # YOUR PRETRAINED MODEL PATH HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyGiuSU56Per",
        "outputId": "fe369ab6-d19f-4bff-acd4-95fe214ba5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-001dbf9e51ad>:28: UserWarning: Argument(s) 'num_shadows_lower, num_shadows_upper' are not valid for transform RandomShadow\n",
            "  A.RandomShadow(\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:58: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "<ipython-input-57-001dbf9e51ad>:65: UserWarning: Argument(s) 'shift_limit' are not valid for transform OpticalDistortion\n",
            "  A.OpticalDistortion(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10/123 images (8.1%)\n",
            "Processed 20/123 images (16.3%)\n",
            "Processed 30/123 images (24.4%)\n",
            "Processed 40/123 images (32.5%)\n",
            "Processed 50/123 images (40.7%)\n",
            "Processed 60/123 images (48.8%)\n",
            "Processed 70/123 images (56.9%)\n",
            "Processed 80/123 images (65.0%)\n",
            "Processed 90/123 images (73.2%)\n",
            "Processed 100/123 images (81.3%)\n",
            "Processed 110/123 images (89.4%)\n",
            "Processed 120/123 images (97.6%)\n",
            "\n",
            "Verifying augmented images...\n",
            "Verified 50/738 images (6.8%)\n",
            "Verified 100/738 images (13.6%)\n",
            "Verified 150/738 images (20.3%)\n",
            "Verified 200/738 images (27.1%)\n",
            "Verified 250/738 images (33.9%)\n",
            "Verified 300/738 images (40.7%)\n",
            "Verified 350/738 images (47.4%)\n",
            "Verified 400/738 images (54.2%)\n",
            "Verified 450/738 images (61.0%)\n",
            "Verified 500/738 images (67.8%)\n",
            "Verified 550/738 images (74.5%)\n",
            "Verified 600/738 images (81.3%)\n",
            "Verified 650/738 images (88.1%)\n",
            "Verified 700/738 images (94.9%)\n",
            "\n",
            "All augmented images are valid!\n"
          ]
        }
      ],
      "source": [
        "# Initialize augmenter\n",
        "print(\"Starting data augmentation...\")\n",
        "augmenter = DataAugmenter(\n",
        "    input_folder='/content/drive/MyDrive/AUP/Ramy',\n",
        "    output_folder='/content/drive/MyDrive/AUP/augmented',\n",
        "    num_augmentations=5\n",
        ")\n",
        "\n",
        "# Run augmentation\n",
        "augmenter.augment_dataset()\n",
        "\n",
        "# Verify results\n",
        "augmenter.verify_augmentations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3rTQulls15e",
        "outputId": "fadb2a4a-acef-43ef-b371-058d004d0038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing product detector...\n",
            "Preparing training data...\n",
            "Found 46 unique families (including Others) in Excel file\n",
            "\n",
            "Matching Report:\n",
            "================\n",
            "Total images processed: 778\n",
            "Successfully matched: 738\n",
            "Unmatched: 40\n",
            "\n",
            "Unique families found: 19\n",
            "Families and their products:\n",
            "\n",
            "PET Boisson Energtique 33 CL:\n",
            "  - ENERGY DRINK 33CL -> PET Energie drink 33 CL\n",
            "  - ENERGY POWER FRUITD 33CL -> PET Energie Power Fruits 33 CL\n",
            "  - PET_Energie_Classique_33cl -> energie classique\n",
            "  - PET_Energie_Classique_33cl(1) -> PET Energie Classique 33CL\n",
            "  - PET_Energie_Menthe_33cl -> energie menthe\n",
            "  - PET_Energie_Miel_33cl -> energie miel\n",
            "  - PET_Energie_Power_Fruits_33cl -> energie power fruits\n",
            "\n",
            "PET Extra Jus 2 L:\n",
            "  - PET_Extra_Ananas_2L -> extra ananas\n",
            "  - extra_ananas30cl -> extra ananas\n",
            "\n",
            "Ramy Kids 110 ML:\n",
            "  - Cannete_Jus_Orange_Abricot_24cl -> Kids Orange Abricot 110 ML\n",
            "  - Cannete_Jus_Orange_Peche_24cl -> Kids Orange Peche 110 ML\n",
            "  - PET_Extra_Orange__Peche_Fraise_2L -> Kids Orange Peche Fraise 110 ML\n",
            "  - PET_Ramy_Orange_Peche_Fraise_1,25L -> Kids Orange Peche Fraise 110 ML\n",
            "  - Pack_Frutty_Kids_Orange_Abricot_20cl -> Kids Orange Abricot 110 ML\n",
            "  - Pack_Kids_Cocktail_125ml -> kids cocktail\n",
            "  - Pack_Kids_Orange_125ml -> kids orange\n",
            "  - Pack_Ramy_Orange_Abricot_1L -> Kids Orange Abricot 110 ML\n",
            "  - Pack_Ramy_Orange_Abricot_2L -> Kids Orange Abricot 110 ML\n",
            "  - Pack_Ramy_Orange_Peche_1L -> Kids Orange Peche 110 ML\n",
            "  - Pack_Ramy_Orange_Peche_20cl -> Kids Orange Peche 110 ML\n",
            "  - Pack_Ramy_Orange_Peche_2L -> Kids Orange Peche 110 ML\n",
            "  - extra_orange_peche_fraise30cl -> Kids Orange Peche Fraise 110 ML\n",
            "\n",
            "PET Extra 30 CL:\n",
            "  - PET_Extra_Orange_2L -> extra orange\n",
            "  - extra_orange_abricot30cl -> PET Extra Or-Abricot 30 CL\n",
            "\n",
            "PET Ramy 1,25 L:\n",
            "  - PET_Ramy_Cerise_1,25L -> PET Ramy Cerise 1.25 L\n",
            "  - PET_Ramy_Citron_1,25L -> PET Ramy Citron 1.25 L\n",
            "  - PET_Ramy_Cocktail_Mangue_1,25L -> PET Ramy Cocktail Mangue 1.25 L\n",
            "  - PET_Ramy_Cocktail_Mure_1,25L -> PET Ramy Cocktail Mure 1.25 L\n",
            "  - PET_Ramy_Fraise_1,25L -> PET Ramy Fraise 1.25 L\n",
            "  - PET_Ramy_Grenadine_1,25L -> PET Ramy Grenadin 1.25 L\n",
            "  - PET_Ramy_Mandarine_1,25L -> PET Ramy Mandarine 1.25 L\n",
            "  - PET_Ramy_Peche_1,25L -> PET Ramy Peche 1.25 L\n",
            "  - PET_Ramy_ananas_1,25L -> PET Ramy Ananas 1.25 L\n",
            "\n",
            "Pack Ramy 1 L:\n",
            "  - PET_Ramy_Cocktail_Raisin_1,25L -> Pack Ramy Cocktail Raisin 1L\n",
            "  - Pack_Ramy_Mandarine_1L -> mandarine\n",
            "\n",
            "Water Fruits 1.25L:\n",
            "  - PET_Ramy_Pomme_Banane_1,25L -> W.F Pomme Banane 1.25L\n",
            "\n",
            "PET Frutty 1.25L:\n",
            "  - PET_Frutty_Orange_Peche_Fraise_2L -> PET Frutty Or-Peche Fraise 1.25 L\n",
            "\n",
            "Pack Frutty 2 L:\n",
            "  - PET_Frutty_Ananas_2L -> frutty ananas\n",
            "  - PET_Frutty_Orange_2L -> frutty orange\n",
            "  - PET_Frutty_Passion_2L -> frutty passion\n",
            "  - Pack_Frutty_Ananas_1L -> frutty ananas\n",
            "  - Pack_Frutty_Orange_1L -> frutty orange\n",
            "\n",
            "PET Frutty Jus 2 L:\n",
            "  - PET_Frutty_Orange_Abricot_2L -> PET Frutty Orange Carotte 2L\n",
            "  - PET_Frutty_Orange_Peche_2L -> PET Frutty Or-Peche 2 L\n",
            "  - Pack_Frutty_Orange_Abricot_1L -> PET Frutty Orange Carotte 2L\n",
            "  - Pack_Frutty_Orange_Peche_1L -> PET Frutty Or-Peche 2 L\n",
            "\n",
            "PET Boisson Malte 33 CL:\n",
            "  - Cannete_Maltee_Ananas_33cl -> PET Malt Ananas 33 CL\n",
            "  - Cannete_Maltee_Miel_33cl -> PET Malt Miel 33 CL\n",
            "  - Cannete_Maltee_Peche_33cl -> PET Malt Peche 33 CL\n",
            "  - PET_Malt_Ananas_33cl -> malt ananas\n",
            "  - PET_Malt_Ananas_33cl(1) -> PET Malt Ananas 33 CL\n",
            "  - PET_Malt_Citron_33cl -> malt citron\n",
            "  - PET_Malt_MFruits_33cl -> PET Malt M/Fruits 33 CL\n",
            "  - PET_Malt_MFruits_33cl(1) -> PET Malt M/Fruits 33 CL\n",
            "  - PET_Malt_Miel_33cl -> malt miel\n",
            "  - PET_Malt_Peche_33cl -> malt peche\n",
            "\n",
            "Canette Jus 24 CL:\n",
            "  - Cannete_Jus_Ananas_24cl -> C.Jus Ananas 24 CL\n",
            "  - Cannete_Jus_Citron_24cl -> C.Jus Citron 24 CL\n",
            "  - Cannete_Jus_Cocktail_24cl -> C.Jus Cocktail 24 CL\n",
            "  - Cannete_Jus_Fraise_24cl -> C.Jus Fraise 24 CL\n",
            "  - Cannete_Jus_Orange_24cl -> C.Jus Orange 24 CL\n",
            "\n",
            "Canette Malte 33 CL:\n",
            "  - Cannete_Maltee_Mojito_33cl -> C.Malt Mojito 33 CL\n",
            "\n",
            "Ramy Kids 125 ML:\n",
            "  - Pack_Kids_Ananas_125ml -> kids ananas\n",
            "  - Pack_Kids_Fraise_125ml -> kids fraise\n",
            "  - Pack_Kids_Peche_125ml -> kids peche\n",
            "\n",
            "Pack Ramy 2 L:\n",
            "  - Pack_Ramy_Ananas_1L -> ananas\n",
            "  - Pack_Ramy_Ananas_20cl -> ananas\n",
            "  - Pack_Ramy_Cocktail_Mangue_1L -> cocktail mangue\n",
            "  - Pack_Ramy_Orange_1L -> orange\n",
            "  - Pack_Ramy_Orange_20cl -> orange\n",
            "\n",
            "Water Fruits 33 CL:\n",
            "  - Pack_Ramy_Orange_Light_1L -> orange light\n",
            "\n",
            "Pack Frutty Kids 20 CL:\n",
            "  - Pack_Frutty_Kids_Ananas_20cl -> frutty kids ananas\n",
            "  - Pack_Frutty_Kids_Cocktail_20cl -> frutty kids cocktail\n",
            "  - Pack_Frutty_Kids_Orange_20cl -> Pack Frutty Kids Or 20 CL\n",
            "  - Pack_Frutty_Kids_Orange_Peche_20cl -> Pack Frutty Kids Or-Peche 20 CL\n",
            "\n",
            "Pack Frutty 1 L:\n",
            "  - Pack_Frutty_Cocktail_1L -> frutty cocktail\n",
            "  - Pack_Frutty_Tropical_1L -> frutty tropical\n",
            "\n",
            "Ramy UP 20 CL:\n",
            "  - UP_Banane_125ml -> up banane\n",
            "  - UP_Banane_20cl -> up banane\n",
            "  - UP_Choco_125ml -> up choco\n",
            "  - UP_Choco_20cl -> up choco\n",
            "  - UP_Fraise_125ml -> up fraise\n",
            "  - UP_Fraise_20cl -> up fraise\n",
            "\n",
            "Unmatched products:\n",
            "  - Cannete_Gazifiee_Agrumes_33cl\n",
            "  - Cannete_Gazifiee_Ananas_33cl\n",
            "  - Cannete_Gazifiee_Citron_Menthe_33cl\n",
            "  - Cannete_Gazifiee_Orange_33cl\n",
            "  - Cannete_Gazifiee_Zest_33cl\n",
            "  - ENERGY RED BUF MENTH 33CL.\n",
            "  - Ifruit13\n",
            "  - Ifruit3\n",
            "  - Ifruit6\n",
            "  - Ifruit8\n",
            "  - MILKY PET 1L\n",
            "  - PET_Ramy_Cocktail_Passion_1,25L\n",
            "  - PET_Ramy_Gazifiee_Orange_33cl\n",
            "  - PET_Ramy_Orange_Abricot_1,25L\n",
            "  - PET_Ramy_Orange_Ananas_1,25L\n",
            "  - PET_Ramy_Orange_Mangue_1,25L\n",
            "  - PET_Ramy_Orange_Sanguine_Grenadine_1,25L\n",
            "  - PET_Ramy_Trio_Fraise_Pomme_Banane_1,25L\n",
            "  - PET_WaterFruits_Agrumes_33cl\n",
            "  - PET_WaterFruits_Ananas_33cl\n",
            "  - PET_WaterFruits_Grenadine_33cl\n",
            "  - PET_WaterFruits_Mojito_33cl\n",
            "  - PET_WaterFruits_Pomme_33cl\n",
            "  - PET_WaterFruits_Pomme_Banane_33cl\n",
            "  - Rouiba1\n",
            "  - Rouiba12\n",
            "  - Rouiba4\n",
            "  - Rouiba6\n",
            "  - Rouiba9\n",
            "  - Toudja1\n",
            "  - Toudja10\n",
            "  - Toudja2\n",
            "  - Toudja4\n",
            "  - Toudja5\n",
            "  - Toudja8\n",
            "  - Toudja9\n",
            "  - ramy LBEN\n",
            "  - ramy LBEN BIF\n",
            "  - ramy canette jus 204ml\n",
            "  - ramy pack 2L\n",
            "\n",
            "Training set size: 590\n",
            "Validation set size: 148\n",
            "Loaded pretrained weights for efficientnet-b2\n",
            "Training classification model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-aca8ee09d259>:146: RuntimeWarning: divide by zero encountered in divide\n",
            "  class_weights = torch.FloatTensor(total_samples / (len(class_counts) * class_counts))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Train Loss: 3.3722\n",
            "Val Loss: 2.9569\n",
            "Overall Val Accuracy: 39.86%\n",
            "\n",
            "Per-class validation accuracy:\n",
            "Canette Jus 24 CL: 0.00%\n",
            "PET Boisson Energtique 33 CL: 87.50%\n",
            "PET Boisson Malte 33 CL: 100.00%\n",
            "PET Extra 30 CL: 75.00%\n",
            "PET Extra Jus 2 L: 0.00%\n",
            "PET Frutty 1.25L: 50.00%\n",
            "PET Frutty Jus 2 L: 0.00%\n",
            "PET Ramy 1,25 L: 64.71%\n",
            "Pack Frutty 1 L: 100.00%\n",
            "Pack Frutty 2 L: 85.71%\n",
            "Pack Frutty Kids 20 CL: 0.00%\n",
            "Pack Ramy 1 L: 0.00%\n",
            "Pack Ramy 2 L: 40.00%\n",
            "Ramy Kids 110 ML: 21.43%\n",
            "Ramy Kids 125 ML: 100.00%\n",
            "Ramy UP 20 CL: 57.14%\n",
            "Water Fruits 1.25L: 100.00%\n",
            "Water Fruits 33 CL: 0.00%\n",
            "Others: 5.00%\n",
            "Epoch 2/5\n",
            "Train Loss: 2.0968\n",
            "Val Loss: 1.4047\n",
            "Overall Val Accuracy: 56.76%\n",
            "\n",
            "Per-class validation accuracy:\n",
            "Canette Jus 24 CL: 100.00%\n",
            "PET Boisson Energtique 33 CL: 87.50%\n",
            "PET Boisson Malte 33 CL: 100.00%\n",
            "PET Extra 30 CL: 100.00%\n",
            "PET Extra Jus 2 L: 50.00%\n",
            "PET Frutty 1.25L: 0.00%\n",
            "PET Frutty Jus 2 L: 33.33%\n",
            "PET Ramy 1,25 L: 100.00%\n",
            "Pack Frutty 1 L: 100.00%\n",
            "Pack Frutty 2 L: 57.14%\n",
            "Pack Frutty Kids 20 CL: 100.00%\n",
            "Pack Ramy 1 L: 0.00%\n",
            "Pack Ramy 2 L: 100.00%\n",
            "Ramy Kids 110 ML: 21.43%\n",
            "Ramy Kids 125 ML: 100.00%\n",
            "Ramy UP 20 CL: 100.00%\n",
            "Water Fruits 1.25L: 100.00%\n",
            "Water Fruits 33 CL: 33.33%\n",
            "Others: 5.00%\n"
          ]
        }
      ],
      "source": [
        "# Initialize and train detector with augmented dataset\n",
        "print(\"Initializing product detector...\")\n",
        "detector = RamyProductDetector(augmented_folder, products_file, model_path=yolo_model_path)\n",
        "detector.prepare_data(epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect products in rayonnage images\n",
        "print(\"Processing rayonnage images...\")\n",
        "results_df = detector.detect_products(rayonnage_folder, output_folder)\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(output_folder, \"detections.xlsx\")\n",
        "results_df.to_excel(results_path, index=False)\n",
        "print(f\"Results saved to {results_path}\")"
      ],
      "metadata": {
        "id": "H1GVFwzij0ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAQk9/3vmgDzwxBozCLX+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}